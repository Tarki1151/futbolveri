import os
import json
import glob
import typing as t
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

import requests
import tempfile
import shutil

import typer
import psycopg
from psycopg.rows import dict_row

app = typer.Typer(help="StatsBomb Stage 1 importer: countries, competitions, seasons, teams, stadiums, matches")

# Defaults
DEFAULT_DATA_DIR = os.environ.get("ARCHIVE_DATA_DIR", os.path.join(os.path.dirname(os.path.dirname(__file__)), "archive", "data"))

# SQL helpers (quoted Turkish identifiers)
SQL_GET_COUNTRY_BY_NAME = 'SELECT id, kod FROM "ulkeler" WHERE isim = %s'
SQL_GET_COUNTRY_BY_CODE = 'SELECT id FROM "ulkeler" WHERE kod = %s'
SQL_INSERT_COUNTRY = 'INSERT INTO "ulkeler" (kod, isim) VALUES (%s, %s) RETURNING id'

SQL_UPSERT_COMPETITION = (
    'INSERT INTO "karsilasmalar" (anahtar, isim, ulke_id, ulusal_mi, uluslararasi_mi)\n'
    'VALUES (%s, %s, %s, %s, %s)\n'
    'ON CONFLICT (anahtar) DO UPDATE SET isim = EXCLUDED.isim, ulke_id = EXCLUDED.ulke_id, ulusal_mi = EXCLUDED.ulusal_mi, uluslararasi_mi = EXCLUDED.uluslararasi_mi\n'
    'RETURNING id'
)

SQL_UPSERT_SEASON = (
    'INSERT INTO "sezonlar" (anahtar, baslangic_yili, bitis_yili)\n'
    'VALUES (%s, %s, %s)\n'
    'ON CONFLICT (anahtar) DO UPDATE SET baslangic_yili = EXCLUDED.baslangic_yili, bitis_yili = EXCLUDED.bitis_yili\n'
    'RETURNING id'
)

SQL_UPSERT_TEAM = (
    'INSERT INTO "takimlar" (anahtar, isim, ulke_id)\n'
    'VALUES (%s, %s, %s)\n'
    'ON CONFLICT (anahtar) DO UPDATE SET isim = EXCLUDED.isim, ulke_id = COALESCE(EXCLUDED.ulke_id, "takimlar".ulke_id)\n'
    'RETURNING id'
)

SQL_SELECT_STADIUM_BY_NAME = 'SELECT id FROM "stadyumlar" WHERE isim = %s'
SQL_INSERT_STADIUM = 'INSERT INTO "stadyumlar" (isim, ulke_id, sehir) VALUES (%s, %s, %s) RETURNING id'

SQL_SELECT_MATCH_ID = (
    'SELECT id FROM "maclar"\n'
    'WHERE yarisma_id = %s AND sezon_id = %s AND mac_tarihi = %s\n'
    'AND ev_sahibi_takim_id = %s AND deplasman_takim_id = %s'
)
SQL_INSERT_MATCH = (
    'INSERT INTO "maclar" (yarisma_id, sezon_id, tur, mac_tarihi, ev_sahibi_takim_id, deplasman_takim_id, stadyum_id, ev_sahibi_goller, deplasman_goller, durum)\n'
    'VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id'
)

# Stage 2 SQL helpers
SQL_SELECT_PLAYER_BY_NAME_DOB = 'SELECT id, uyruk_id, mevki FROM "oyuncular" WHERE isim = %s AND (dogum_tarihi IS NOT DISTINCT FROM %s)'
SQL_INSERT_PLAYER = (
    'INSERT INTO "oyuncular" (isim, ilk_isim, soyisim, dogum_tarihi, uyruk_id, mevki)\n'
    'VALUES (%s, %s, %s, %s, %s, %s) RETURNING id'
)
SQL_UPDATE_PLAYER_META = (
    "UPDATE \"oyuncular\" SET uyruk_id = COALESCE(%s, uyruk_id), mevki = CASE WHEN mevki = 'bilinmeyen' THEN %s ELSE mevki END WHERE id = %s"
)
SQL_SELECT_PLAYER_BY_NAME = 'SELECT id, dogum_tarihi FROM "oyuncular" WHERE isim = %s ORDER BY (dogum_tarihi IS NULL) DESC, id ASC LIMIT 1'
SQL_UPDATE_PLAYER_DOB_META = (
    "UPDATE \"oyuncular\" SET dogum_tarihi = CASE WHEN dogum_tarihi IS NULL THEN %s ELSE dogum_tarihi END, "
    "uyruk_id = COALESCE(%s, uyruk_id), mevki = CASE WHEN mevki = 'bilinmeyen' THEN %s ELSE mevki END WHERE id = %s"
)
SQL_SELECT_TEAM_ID_BY_KEY = 'SELECT id FROM "takimlar" WHERE anahtar = %s'
SQL_SELECT_SEASON_ID_BY_KEY = 'SELECT id FROM "sezonlar" WHERE anahtar = %s'
SQL_SELECT_SQUAD = 'SELECT id FROM "kadrolar" WHERE takim_id = %s AND oyuncu_id = %s AND sezon_id = %s'
SQL_INSERT_SQUAD = 'INSERT INTO "kadrolar" (takim_id, oyuncu_id, sezon_id, forma_numarasi, aktif_mi) VALUES (%s, %s, %s, %s, %s) RETURNING id'
SQL_SELECT_LINEUP = 'SELECT id FROM "dizilisler" WHERE mac_id = %s AND takim_id = %s AND oyuncu_id = %s'
SQL_INSERT_LINEUP = 'INSERT INTO "dizilisler" (mac_id, takim_id, oyuncu_id, mevki, ilk_onbir_mi, oynama_dakikasi) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id'

# Stage 3 SQL helpers (events)
SQL_SELECT_EVENT = (
    'SELECT id FROM "olaylar" WHERE mac_id = %s AND takim_id IS NOT DISTINCT FROM %s AND oyuncu_id IS NOT DISTINCT FROM %s '
    'AND olay_tipi = %s AND dakika IS NOT DISTINCT FROM %s AND ayrinti IS NOT DISTINCT FROM %s'
)
SQL_INSERT_EVENT = 'INSERT INTO "olaylar" (mac_id, oyuncu_id, takim_id, olay_tipi, dakika, ayrinti) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id'

# Import progress tracking table for resumable imports
SQL_CREATE_PROGRESS_TABLE = (
    'CREATE TABLE IF NOT EXISTS "islenen_dosyalar" ('
    '  "id" INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,'
    '  "kategori" varchar(50) NOT NULL,'
    '  "dosya" varchar(255) NOT NULL,'
    '  "islenme_tarihi" timestamp DEFAULT (CURRENT_TIMESTAMP),'
    '  UNIQUE("kategori", "dosya")'
    ')'
)
SQL_LIST_PROCESSED = 'SELECT dosya FROM "islenen_dosyalar" WHERE kategori = %s'
SQL_MARK_PROCESSED = (
    'INSERT INTO "islenen_dosyalar" (kategori, dosya, islenme_tarihi) VALUES (%s, %s, CURRENT_TIMESTAMP) '
    'ON CONFLICT (kategori, dosya) DO UPDATE SET islenme_tarihi = EXCLUDED.islenme_tarihi'
)
SQL_RESET_PROCESSED = 'DELETE FROM "islenen_dosyalar" WHERE kategori = %s'

# StatsBomb match_id -> DB mac_id mapping table
SQL_CREATE_SB_MATCH_MAP = (
    'CREATE TABLE IF NOT EXISTS "sb_mac_haritasi" ('
    '  "sb_match_id" BIGINT PRIMARY KEY,'
    '  "mac_id" INT NOT NULL REFERENCES "maclar"(id)'
    ')'
)
SQL_UPSERT_SB_MATCH_MAP = (
    'INSERT INTO "sb_mac_haritasi" (sb_match_id, mac_id) VALUES (%s, %s) '
    'ON CONFLICT (sb_match_id) DO UPDATE SET mac_id = EXCLUDED.mac_id'
)


def get_conn():
    dsn = os.environ.get("DATABASE_URL")
    if dsn:
        return psycopg.connect(dsn)
    # Fallback to libpq envs
    return psycopg.connect(
        host=os.environ.get("PGHOST", "localhost"),
        port=os.environ.get("PGPORT", "5432"),
        user=os.environ.get("PGUSER"),
        password=os.environ.get("PGPASSWORD"),
        dbname=os.environ.get("PGDATABASE", "futbol"),
    )


def load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_country_map() -> dict:
    here = os.path.dirname(__file__)
    path = os.path.join(here, "mappings", "country_iso3.json")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_position_map() -> dict:
    here = os.path.dirname(__file__)
    path = os.path.join(here, "mappings", "position_map.json")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def parse_season_name(name: str) -> t.Tuple[int, int]:
    # e.g. "2015/2016" -> (2015, 2016)
    parts = name.split("/")
    if len(parts) == 2:
        return int(parts[0]), int(parts[1])
    # fallback: YYYY -> (YYYY, YYYY+1)
    y = int(name[:4])
    return y, y + 1


def ensure_country(cur, name: str, country_map: dict) -> t.Optional[int]:
    if not name:
        return None
    # Normalize whitespace (convert NBSP to space, collapse multiple spaces)
    normalized = " ".join((name or "").replace("\u00A0", " ").split())
    # Try exact name match first
    cur.execute(SQL_GET_COUNTRY_BY_NAME, (name,))
    row = cur.fetchone()
    if row:
        return row[0]
    # Try normalized name if different
    if normalized != name:
        cur.execute(SQL_GET_COUNTRY_BY_NAME, (normalized,))
        rowN = cur.fetchone()
        if rowN:
            return rowN[0]
    # Lookup mapping using original or normalized form
    code = country_map.get(name) or country_map.get(normalized)
    if not code:
        typer.echo(f"[WARN] ISO3 not found for country '{name}'. Skipping. Update mappings/country_iso3.json")
        return None
    # check by code too (do not rename existing record if name differs)
    cur.execute(SQL_GET_COUNTRY_BY_CODE, (code,))
    row2 = cur.fetchone()
    if row2:
        return row2[0]
    # Insert with normalized name to avoid odd unicode spacing in DB
    cur.execute(SQL_INSERT_COUNTRY, (code, normalized))
    return cur.fetchone()[0]


def _split_name(full: str) -> t.Tuple[t.Optional[str], t.Optional[str]]:
    parts = (full or "").split()
    if not parts:
        return None, None
    if len(parts) == 1:
        return parts[0], None
    return " ".join(parts[:-1]), parts[-1]


def ensure_player(cur, name: str, birth_date: t.Optional[str], nationality: t.Optional[str], mevki: t.Optional[str], country_map: dict) -> int:
    dob = None
    if birth_date:
        try:
            dob = datetime.fromisoformat(birth_date).date()
        except Exception:
            dob = None
    cur.execute(SQL_SELECT_PLAYER_BY_NAME_DOB, (name, dob))
    row = cur.fetchone()
    uyruk_id = ensure_country(cur, nationality, country_map) if nationality else None
    mevki_val = mevki or 'bilinmeyen'
    if row:
        pid = row[0]
        # update nationality or position if missing
        cur.execute(SQL_UPDATE_PLAYER_META, (uyruk_id, mevki_val, pid))
        return pid
    # Fallback: try match by name only to reduce duplicates. If existing DOB is NULL and we have DOB, set it.
    cur.execute(SQL_SELECT_PLAYER_BY_NAME, (name,))
    row2 = cur.fetchone()
    if row2:
        pid, existing_dob = row2[0], row2[1]
        if existing_dob is None:
            cur.execute(SQL_UPDATE_PLAYER_DOB_META, (dob, uyruk_id, mevki_val, pid))
        else:
            cur.execute(SQL_UPDATE_PLAYER_META, (uyruk_id, mevki_val, pid))
        return pid
    first, last = _split_name(name)
    cur.execute(SQL_INSERT_PLAYER, (name, first, last, dob, uyruk_id, mevki_val))
    return cur.fetchone()[0]


def _process_single_event_file(path: str, mac_id: int, bname: str, team_cache: dict[str, int], player_cache: dict[str, int]) -> tuple[str, int]:
    """Worker: process a single events JSON file.

    Returns (basename, inserted_count).
    """
    inserted = 0
    with get_conn() as conn:
        with conn.cursor() as cur:
            evs = load_json(path)
            # Build map of assisted shots for quick lookup
            shot_assist: dict[str, dict] = {}
            for ev in evs:
                tinfo = (ev.get("type") or {}).get("name")
                if tinfo == "Pass":
                    pinfo = ev.get("pass") or {}
                    asid = pinfo.get("assisted_shot_id")
                    if asid:
                        shot_assist[str(asid)] = ev

            for ev in evs:
                etype = (ev.get("type") or {}).get("name")
                minute = ev.get("minute")
                team_fk = None
                team_id = (ev.get("team") or {}).get("id")
                if team_id is not None:
                    team_fk = team_cache.get(str(team_id))
                player_name = (ev.get("player") or {}).get("name")
                pid = player_cache.get(player_name) if player_name else None

                # Goals
                if etype == "Shot" and (ev.get("shot") or {}).get("outcome", {}).get("name") == "Goal":
                    if pid is not None:
                        detail = (ev.get("shot") or {}).get("body_part", {}).get("name")
                        cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'gol', minute, detail))
                        if not cur.fetchone():
                            cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'gol', minute, detail))
                            _ = cur.fetchone()[0]
                            inserted += 1
                    # Assist from prior pass
                    sid = ev.get("id")
                    apass = shot_assist.get(str(sid))
                    if apass is not None:
                        asst_player = (apass.get("player") or {}).get("name")
                        if asst_player:
                            apid = player_cache.get(asst_player)
                            cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, apid, 'asist', minute, None))
                            if not cur.fetchone():
                                cur.execute(SQL_INSERT_EVENT, (mac_id, apid, team_fk, 'asist', minute, None))
                                _ = cur.fetchone()[0]
                                inserted += 1

                # Cards from fouls or bad behaviour
                if etype == "Foul Committed" and pid is not None:
                    card = (ev.get("foul_committed") or {}).get("card", {})
                    card_name = card.get("name")
                    if card_name:
                        cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'kart', minute, card_name))
                        if not cur.fetchone():
                            cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'kart', minute, card_name))
                            _ = cur.fetchone()[0]
                            inserted += 1
                if etype == "Bad Behaviour" and pid is not None:
                    card = (ev.get("bad_behaviour") or {}).get("card", {})
                    card_name = card.get("name")
                    if card_name:
                        cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'kart', minute, card_name))
                        if not cur.fetchone():
                            cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'kart', minute, card_name))
                            _ = cur.fetchone()[0]
                            inserted += 1

                # Substitutions
                if etype == "Substitution" and pid is not None:
                    sub = ev.get("substitution") or {}
                    rep = (sub.get("replacement") or {}).get("name")
                    detail = f"in: {rep}" if rep else None
                    cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'degisiklik', minute, detail))
                    if not cur.fetchone():
                        cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'degisiklik', minute, detail))
                        _ = cur.fetchone()[0]
                        inserted += 1

            # Mark file processed and commit progress per file
            cur.execute(SQL_MARK_PROCESSED, ('events', bname))
            conn.commit()
    return bname, inserted


def _map_position(raw: t.Optional[str], pos_map: dict) -> str:
    if not raw:
        return 'bilinmeyen'
    return pos_map.get(raw, pos_map.get(raw.title(), 'bilinmeyen'))


def build_match_map(conn, data_dir: str) -> dict:
    """Return dict[statsbomb_match_id] -> db_mac_id using matches files and existing DB rows."""
    match_map: dict[int, int] = {}
    with conn.cursor() as cur:
        # Caches
        comp_cache: dict[str, int] = {}
        cur.execute('SELECT id, anahtar FROM "karsilasmalar"')
        for r in cur.fetchall():
            comp_cache[str(r[1])] = r[0]
        season_cache: dict[str, int] = {}
        cur.execute('SELECT id, anahtar FROM "sezonlar"')
        for r in cur.fetchall():
            season_cache[str(r[1])] = r[0]
        team_cache: dict[str, int] = {}
        cur.execute('SELECT id, anahtar FROM "takimlar"')
        for r in cur.fetchall():
            team_cache[str(r[1])] = r[0]

        for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
            arr = load_json(path)
            for m in arr:
                sb_match_id = int(m["match_id"]) if m.get("match_id") is not None else None
                comp_key = str(m.get("competition", {}).get("competition_id")) if m.get("competition") else None
                season_key = str(m.get("season", {}).get("season_id")) if m.get("season") else None
                if not (sb_match_id and comp_key and season_key):
                    continue
                comp_id = comp_cache.get(comp_key)
                season_id = season_cache.get(season_key)
                home_id = (m.get("home_team") or {}).get("home_team_id")
                away_id = (m.get("away_team") or {}).get("away_team_id")
                if None in (comp_id, season_id, home_id, away_id):
                    continue
                home_fk = team_cache.get(str(home_id))
                away_fk = team_cache.get(str(away_id))
                if home_fk is None or away_fk is None:
                    continue
                date_str = m.get("match_date")
                ko = (m.get("kick_off") or "00:00:00.000").split(".")[0]
                mac_ts = datetime.fromisoformat(f"{date_str}T{ko}")
                cur.execute(SQL_SELECT_MATCH_ID, (comp_id, season_id, mac_ts, home_fk, away_fk))
                r = cur.fetchone()
                if r:
                    match_map[sb_match_id] = r[0]
    return match_map


def _get_match_map(conn, data_dir: str) -> dict[int, int]:
    """Prefer mapping from sb_mac_haritasi; fallback to scanning matches files if table/rows absent."""
    try:
        with conn.cursor() as cur:
            cur.execute('SELECT sb_match_id, mac_id FROM "sb_mac_haritasi"')
            rows = cur.fetchall()
            if rows:
                return {int(r[0]): int(r[1]) for r in rows}
    except Exception:
        # table may not exist yet
        pass
    return build_match_map(conn, data_dir)


@app.command()
def scan_countries(data_dir: str = DEFAULT_DATA_DIR):
    """Scan JSON and list distinct country names found in competitions, matches, teams, stadiums."""
    countries = set()
    # competitions
    comp_path = os.path.join(data_dir, "competitions.json")
    comps = load_json(comp_path)
    for c in comps:
        if c.get("country_name"):
            countries.add(c["country_name"])
    # matches
    for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
        arr = load_json(path)
        for m in arr:
            for key in ("home_team", "away_team"):
                country = (((m.get(key) or {}).get("country") or {}).get("name"))
                if country:
                    countries.add(country)
            st_country = (((m.get("stadium") or {}).get("country") or {}).get("name"))
            if st_country:
                countries.add(st_country)
    # output
    typer.echo("Found countries (sample up to 100):")
    for name in sorted(list(countries))[:100]:
        typer.echo(f" - {name}")
    typer.echo(f"Total distinct: {len(countries)}")


@app.command()
def import_countries(data_dir: str = DEFAULT_DATA_DIR):
    """Import countries into ulkeler using mappings/country_iso3.json"""
    country_map = load_country_map()
    countries = set()
    comp_path = os.path.join(data_dir, "competitions.json")
    comps = load_json(comp_path)
    for c in comps:
        if c.get("country_name"):
            countries.add(c["country_name"])
    for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
        arr = load_json(path)
        for m in arr:
            for key in ("home_team", "away_team"):
                country = (((m.get(key) or {}).get("country") or {}).get("name"))
                if country:
                    countries.add(country)
            st_country = (((m.get("stadium") or {}).get("country") or {}).get("name"))
            if st_country:
                countries.add(st_country)
    with get_conn() as conn:
        with conn.cursor() as cur:
            for name in sorted(countries):
                cid = ensure_country(cur, name, country_map)
                if cid:
                    typer.echo(f"[OK] {name}")
            conn.commit()
    typer.echo("Countries import complete.")


@app.command()
def import_competitions_and_seasons(data_dir: str = DEFAULT_DATA_DIR):
    """Import competitions (karsilasmalar) and seasons (sezonlar) from competitions.json"""
    comps = load_json(os.path.join(data_dir, "competitions.json"))
    country_map = load_country_map()
    with get_conn() as conn:
        with conn.cursor() as cur:
            seen_seasons = set()
            for c in comps:
                comp_id = str(c["competition_id"])  # as text for anahtar
                comp_name = c["competition_name"]
                country_name = c.get("country_name")
                is_int = bool(c.get("competition_international"))
                country_id = ensure_country(cur, country_name, country_map) if country_name else None
                cur.execute(SQL_UPSERT_COMPETITION, (
                    comp_id,
                    comp_name,
                    country_id,
                    not is_int,
                    is_int,
                ))
                comp_row_id = cur.fetchone()[0]
                # season
                season_id = str(c["season_id"])  # as text
                season_name = c["season_name"]
                if season_id not in seen_seasons:
                    y1, y2 = parse_season_name(season_name)
                    cur.execute(SQL_UPSERT_SEASON, (season_id, y1, y2))
                    seen_seasons.add(season_id)
                    _ = cur.fetchone()[0]
            conn.commit()
    typer.echo("Competitions and seasons import complete.")


@app.command()
def import_teams(data_dir: str = DEFAULT_DATA_DIR):
    """Import unique teams discovered in matches files."""
    country_map = load_country_map()
    teams: dict[str, dict] = {}
    for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
        arr = load_json(path)
        for m in arr:
            for side, key in (("home", "home_team"), ("away", "away_team")):
                tinfo = m.get(key) or {}
                tid = tinfo.get(f"{side}_team_id") or tinfo.get("team_id")
                tname = tinfo.get(f"{side}_team_name") or tinfo.get("team_name")
                country = ((tinfo.get("country") or {}).get("name"))
                if tid and tname:
                    teams[str(tid)] = {"name": tname, "country": country}
    with get_conn() as conn:
        with conn.cursor() as cur:
            for tid, info in sorted(teams.items(), key=lambda x: int(x[0])):
                country_id = ensure_country(cur, info.get("country"), country_map) if info.get("country") else None
                cur.execute(SQL_UPSERT_TEAM, (tid, info["name"], country_id))
                _ = cur.fetchone()[0]
            conn.commit()
    typer.echo(f"Imported/updated {len(teams)} teams.")


@app.command()
def import_stadiums(data_dir: str = DEFAULT_DATA_DIR):
    """Import stadiums referenced in matches."""
    country_map = load_country_map()
    st_map: dict[str, dict] = {}
    for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
        arr = load_json(path)
        for m in arr:
            st = m.get("stadium") or {}
            name = st.get("name")
            if not name:
                continue
            country_name = ((st.get("country") or {}).get("name"))
            st_map[name] = {"country": country_name, "city": None}
    with get_conn() as conn:
        with conn.cursor() as cur:
            for name, info in sorted(st_map.items()):
                cur.execute(SQL_SELECT_STADIUM_BY_NAME, (name,))
                row = cur.fetchone()
                if row:
                    continue
                country_id = ensure_country(cur, info.get("country"), country_map) if info.get("country") else None
                cur.execute(SQL_INSERT_STADIUM, (name, country_id, info.get("city")))
            conn.commit()
    typer.echo(f"Imported/updated {len(st_map)} stadiums.")


@app.command()
def import_matches(data_dir: str = DEFAULT_DATA_DIR):
    """Import matches from matches/*/*.json"""
    with get_conn() as conn:
        with conn.cursor(row_factory=dict_row) as cur:
            # Ensure sb_mac_haritasi exists for mapping
            cur.execute(SQL_CREATE_SB_MATCH_MAP)
            # Build quick lookup caches: competition_id -> id, season_id -> id, team_id->id, stadium_name->id
            comp_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "karsilasmalar"')
            for r in cur.fetchall():
                comp_cache[r["anahtar"]] = r["id"]
            season_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "sezonlar"')
            for r in cur.fetchall():
                season_cache[r["anahtar"]] = r["id"]
            team_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "takimlar"')
            for r in cur.fetchall():
                team_cache[r["anahtar"]] = r["id"]
            stadium_cache: dict[str, int] = {}
            cur.execute('SELECT id, isim FROM "stadyumlar"')
            for r in cur.fetchall():
                stadium_cache[r["isim"]] = r["id"]

            inserted = 0
            for path in glob.glob(os.path.join(data_dir, "matches", "*", "*.json")):
                arr = load_json(path)
                for m in arr:
                    # StatsBomb match id if present
                    sb_id_val = m.get("match_id")
                    sb_match_id = int(sb_id_val) if sb_id_val is not None else None
                    comp_key = str(m["competition"]["competition_id"]) if m.get("competition") else None
                    season_key = str(m["season"]["season_id"]) if m.get("season") else None
                    if not (comp_key and season_key):
                        continue
                    comp_id = comp_cache.get(comp_key)
                    season_id = season_cache.get(season_key)
                    if comp_id is None or season_id is None:
                        continue

                    # teams
                    home_id = m.get("home_team", {}).get("home_team_id")
                    away_id = m.get("away_team", {}).get("away_team_id")
                    if not (home_id and away_id):
                        continue
                    home_fk = team_cache.get(str(home_id))
                    away_fk = team_cache.get(str(away_id))
                    if home_fk is None or away_fk is None:
                        continue

                    # date/time
                    date_str = m.get("match_date")  # YYYY-MM-DD
                    ko = (m.get("kick_off") or "00:00:00.000").split(".")[0]  # HH:MM:SS
                    mac_ts = datetime.fromisoformat(f"{date_str}T{ko}")

                    # stadium
                    st_name = (m.get("stadium") or {}).get("name")
                    st_id = stadium_cache.get(st_name) if st_name else None

                    tur = m.get("match_week") or None
                    hs = m.get("home_score") or 0
                    as_ = m.get("away_score") or 0
                    durum = "bitti"

                    # skip if exists
                    cur.execute(SQL_SELECT_MATCH_ID, (comp_id, season_id, mac_ts, home_fk, away_fk))
                    r = cur.fetchone()
                    if r:
                        mac_row_id = int(r["id"])  # dict_row cursor: access by column name
                        if sb_match_id is not None:
                            cur.execute(SQL_UPSERT_SB_MATCH_MAP, (sb_match_id, mac_row_id))
                        continue

                    cur.execute(SQL_INSERT_MATCH, (comp_id, season_id, tur, mac_ts, home_fk, away_fk, st_id, hs, as_, durum))
                    mac_row_id = int(cur.fetchone()["id"])
                    if sb_match_id is not None:
                        cur.execute(SQL_UPSERT_SB_MATCH_MAP, (sb_match_id, mac_row_id))
                    inserted += 1
            conn.commit()
    typer.echo(f"Inserted {inserted} matches (others already present or skipped).")


@app.command()
def import_players_and_squads(data_dir: str = DEFAULT_DATA_DIR):
    """Import players (oyuncular) and squads (kadrolar) using lineups files for season membership."""
    country_map = load_country_map()
    pos_map = load_position_map()
    with get_conn() as conn:
        match_map = _get_match_map(conn, data_dir)
        with conn.cursor() as cur:
            # simple team cache
            team_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "takimlar"')
            for r in cur.fetchall():
                team_cache[str(r[1])] = r[0]
            # season cache from matches
            season_of_match: dict[int, int] = {}
            cur.execute('SELECT id, sezon_id FROM "maclar"')
            for r in cur.fetchall():
                season_of_match[int(r[0])] = int(r[1]) if r[1] is not None else None

            added_squads = 0
            added_players = 0
            for path in glob.glob(os.path.join(data_dir, "lineups", "*.json")):
                sb_id_str = os.path.splitext(os.path.basename(path))[0]
                try:
                    sb_match_id = int(sb_id_str)
                except ValueError:
                    continue
                mac_id = match_map.get(sb_match_id)
                if mac_id is None:
                    continue
                sezon_id = season_of_match.get(mac_id)
                if sezon_id is None:
                    continue
                data = load_json(path)
                for team_block in data:
                    # Some lineup files have team fields at the top level (team_id/team_name) without a nested 'team' object.
                    # Support both structures.
                    tinfo = team_block.get("team") or team_block
                    tid = tinfo.get("team_id") or tinfo.get("id")
                    team_fk = team_cache.get(str(tid)) if tid is not None else None
                    for p in (team_block.get("lineup") or []):
                        name = p.get("player_name") or p.get("name")
                        dob = p.get("birth_date")
                        nationality = ((p.get("country") or {}).get("name"))
                        # position from lineup (first positions entry)
                        positions = p.get("positions") or []
                        raw_pos = None
                        if positions:
                            raw_pos = positions[0].get("position")
                        mevki = _map_position(raw_pos, pos_map)
                        pid = ensure_player(cur, name, dob, nationality, mevki, country_map)
                        if pid:
                            added_players += 1
                            if team_fk and sezon_id:
                                cur.execute(SQL_SELECT_SQUAD, (team_fk, pid, sezon_id))
                                if not cur.fetchone():
                                    jn = p.get("jersey_number")
                                    cur.execute(SQL_INSERT_SQUAD, (team_fk, pid, sezon_id, jn, True))
                                    _ = cur.fetchone()[0]
                                    added_squads += 1
            conn.commit()
    typer.echo(f"Imported players (new/updated approx): {added_players}, squads added: {added_squads}")


@app.command()
def import_lineups(data_dir: str = DEFAULT_DATA_DIR):
    """Import lineups into dizilisler (starting XI and bench) from lineups files."""
    country_map = load_country_map()
    pos_map = load_position_map()
    with get_conn() as conn:
        match_map = _get_match_map(conn, data_dir)
        with conn.cursor() as cur:
            # caches
            team_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "takimlar"')
            for r in cur.fetchall():
                team_cache[str(r[1])] = r[0]

            inserted = 0
            for path in glob.glob(os.path.join(data_dir, "lineups", "*.json")):
                sb_id_str = os.path.splitext(os.path.basename(path))[0]
                try:
                    sb_match_id = int(sb_id_str)
                except ValueError:
                    continue
                mac_id = match_map.get(sb_match_id)
                if mac_id is None:
                    continue
                data = load_json(path)
                for team_block in data:
                    # Support both nested 'team' object and top-level team fields
                    tinfo = team_block.get("team") or team_block
                    tid = tinfo.get("team_id") or tinfo.get("id")
                    team_fk = team_cache.get(str(tid)) if tid is not None else None
                    if team_fk is None:
                        continue
                    # starters
                    for p in (team_block.get("lineup") or []):
                        name = p.get("player_name") or p.get("name")
                        dob = p.get("birth_date")
                        nationality = ((p.get("country") or {}).get("name"))
                        positions = p.get("positions") or []
                        raw_pos = positions[0].get("position") if positions else None
                        mevki = _map_position(raw_pos, pos_map)
                        pid = ensure_player(cur, name, dob, nationality, mevki, country_map)
                        cur.execute(SQL_SELECT_LINEUP, (mac_id, team_fk, pid))
                        if not cur.fetchone():
                            cur.execute(SQL_INSERT_LINEUP, (mac_id, team_fk, pid, mevki, True, 0))
                            _ = cur.fetchone()[0]
                            inserted += 1
                    # bench if present
                    for p in (team_block.get("substitutes") or []):
                        name = p.get("player_name") or p.get("name")
                        dob = p.get("birth_date")
                        nationality = ((p.get("country") or {}).get("name"))
                        raw_pos = p.get("position")
                        mevki = _map_position(raw_pos, pos_map)
                        pid = ensure_player(cur, name, dob, nationality, mevki, country_map)
                        cur.execute(SQL_SELECT_LINEUP, (mac_id, team_fk, pid))
                        if not cur.fetchone():
                            cur.execute(SQL_INSERT_LINEUP, (mac_id, team_fk, pid, mevki, False, 0))
                            _ = cur.fetchone()[0]
                            inserted += 1
            conn.commit()
    typer.echo(f"Inserted {inserted} lineup rows.")


@app.command()
def import_events(
    data_dir: str = DEFAULT_DATA_DIR,
    limit: t.Optional[int] = typer.Option(None, help="Process at most this many event files (per run)."),
    resume: bool = typer.Option(True, "--resume/--no-resume", help="Skip files already processed in prior runs."),
    reset: bool = typer.Option(False, help="Clear processed marks for events before importing."),
    start_from: t.Optional[int] = typer.Option(None, help="Start from this StatsBomb match_id (inclusive)."),
    only_match: t.Optional[int] = typer.Option(None, help="Only process this single StatsBomb match_id."),
    workers: int = typer.Option(1, "--workers", "-w", help="Number of parallel worker processes for events import"),
):
    """Import key events (gol/asist/kart/degisiklik) from events files into 'olaylar'.

    Resumable: commits after each file and records processed files in 'islenen_dosyalar'.
    """
    with get_conn() as conn:
        match_map = _get_match_map(conn, data_dir)
        with conn.cursor() as cur:
            # Ensure progress table exists and optionally reset
            cur.execute(SQL_CREATE_PROGRESS_TABLE)
            conn.commit()
            if reset:
                cur.execute(SQL_RESET_PROCESSED, ('events',))
                conn.commit()
            # caches
            country_map = load_country_map()
            pos_map = load_position_map()
            team_cache: dict[str, int] = {}
            cur.execute('SELECT id, anahtar FROM "takimlar"')
            for r in cur.fetchall():
                team_cache[str(r[1])] = r[0]

            inserted = 0
            files_done = 0
            # Support both events/*.json and events/*/*.json structures
            event_files = []
            event_files.extend(glob.glob(os.path.join(data_dir, "events", "*.json")))
            event_files.extend(glob.glob(os.path.join(data_dir, "events", "*", "*.json")))

            # Deterministic order by numeric match_id when possible
            def _key(p: str):
                b = os.path.splitext(os.path.basename(p))[0]
                return (0, int(b)) if b.isdigit() else (1, b)
            event_files = sorted(event_files, key=_key)

            processed: set[str] = set()
            if resume:
                cur.execute(SQL_LIST_PROCESSED, ('events',))
                processed = {row[0] for row in cur.fetchall()}

            # Build list of files to process this run, respecting filters and resume
            to_process: list[tuple[str, int, int, str]] = []  # (path, sb_match_id, mac_id, basename)
            for path in event_files:
                base = os.path.splitext(os.path.basename(path))[0]
                bname = os.path.basename(path)
                if only_match is not None and base != str(only_match):
                    continue
                if start_from is not None and base.isdigit() and int(base) < start_from:
                    continue
                if resume and bname in processed:
                    continue
                try:
                    sb_match_id = int(base)
                except ValueError:
                    continue
                mac_id = match_map.get(sb_match_id)
                if mac_id is None:
                    continue
                to_process.append((path, sb_match_id, mac_id, bname))

            if limit is not None:
                to_process = to_process[:limit]

            total = len(to_process)

            if workers and workers > 1 and total > 0:
                typer.echo(f"Scheduling {total} files across {workers} workers...")
                # Use separate DB connection per worker
                # Preload player cache: name -> id, to avoid concurrent updates in workers
                player_cache: dict[str, int] = {}
                cur.execute('SELECT id, isim FROM "oyuncular"')
                for row in cur.fetchall():
                    pid, name = int(row[0]), row[1]
                    if name and name not in player_cache:
                        player_cache[name] = pid
                with ProcessPoolExecutor(max_workers=workers) as pool:
                    futures = [
                        pool.submit(_process_single_event_file, path, mac_id, bname, team_cache, player_cache)
                        for (path, _sbid, mac_id, bname) in to_process
                    ]
                    for fut in as_completed(futures):
                        try:
                            bname_done, ins = fut.result()
                            files_done += 1
                            inserted += ins
                            typer.echo(f"Done {files_done}/{total}: {bname_done} (+{ins})")
                        except Exception as e:
                            typer.echo(f"[ERROR] Worker failed: {e}")
            else:
                for idx, (path, sb_match_id, mac_id, bname) in enumerate(to_process, start=1):
                    typer.echo(f"Processing {idx} of {total}: {bname}")
                    evs = load_json(path)
                    # Build map of assisted shots
                    shot_assist: dict[str, dict] = {}
                    for ev in evs:
                        tinfo = (ev.get("type") or {}).get("name")
                        if tinfo == "Pass":
                            pinfo = ev.get("pass") or {}
                            asid = pinfo.get("assisted_shot_id")
                            if asid:
                                shot_assist[str(asid)] = ev

                    for ev in evs:
                        etype = (ev.get("type") or {}).get("name")
                        minute = ev.get("minute")
                        team_fk = None
                        team_id = (ev.get("team") or {}).get("id")
                        if team_id is not None:
                            team_fk = team_cache.get(str(team_id))
                        player_name = (ev.get("player") or {}).get("name")
                        pid = None
                        if player_name:
                            # Unknown DOB/nationality/position in events; ensure by name only
                            pid = ensure_player(cur, player_name, None, None, None, country_map)

                        # Goals
                        if etype == "Shot" and (ev.get("shot") or {}).get("outcome", {}).get("name") == "Goal":
                            if pid is not None:
                                detail = (ev.get("shot") or {}).get("body_part", {}).get("name")
                                cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'gol', minute, detail))
                                if not cur.fetchone():
                                    cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'gol', minute, detail))
                                    _ = cur.fetchone()[0]
                                    inserted += 1
                            # Assist from prior pass
                            sid = ev.get("id")
                            apass = shot_assist.get(str(sid))
                            if apass is not None:
                                asst_player = (apass.get("player") or {}).get("name")
                                if asst_player:
                                    apid = ensure_player(cur, asst_player, None, None, None, country_map)
                                    cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, apid, 'asist', minute, None))
                                    if not cur.fetchone():
                                        cur.execute(SQL_INSERT_EVENT, (mac_id, apid, team_fk, 'asist', minute, None))
                                        _ = cur.fetchone()[0]
                                        inserted += 1

                        # Cards from fouls or bad behaviour
                        if etype == "Foul Committed" and pid is not None:
                            card = (ev.get("foul_committed") or {}).get("card", {})
                            card_name = card.get("name")
                            if card_name:
                                cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'kart', minute, card_name))
                                if not cur.fetchone():
                                    cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'kart', minute, card_name))
                                    _ = cur.fetchone()[0]
                                    inserted += 1
                        if etype == "Bad Behaviour" and pid is not None:
                            card = (ev.get("bad_behaviour") or {}).get("card", {})
                            card_name = card.get("name")
                            if card_name:
                                cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'kart', minute, card_name))
                                if not cur.fetchone():
                                    cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'kart', minute, card_name))
                                    _ = cur.fetchone()[0]
                                    inserted += 1

                        # Substitutions
                        if etype == "Substitution" and pid is not None:
                            sub = ev.get("substitution") or {}
                            rep = (sub.get("replacement") or {}).get("name")
                            detail = f"in: {rep}" if rep else None
                            cur.execute(SQL_SELECT_EVENT, (mac_id, team_fk, pid, 'degisiklik', minute, detail))
                            if not cur.fetchone():
                                cur.execute(SQL_INSERT_EVENT, (mac_id, pid, team_fk, 'degisiklik', minute, detail))
                                _ = cur.fetchone()[0]
                                inserted += 1

                    # Mark file processed and commit progress per file
                    cur.execute(SQL_MARK_PROCESSED, ('events', bname))
                    conn.commit()
                    files_done += 1
                    # limit is already applied to to_process

    typer.echo(f"Processed {files_done} files. Inserted {inserted} events (others already present or skipped).")


def _github_get_json(url: str, token: t.Optional[str]) -> dict:
    headers = {"Accept": "application/vnd.github+json"}
    if token:
        headers["Authorization"] = f"Bearer {token}"
    r = requests.get(url, headers=headers, timeout=30)
    if r.status_code != 200:
        raise RuntimeError(f"GitHub API error {r.status_code} for {url}: {r.text[:200]}")
    return r.json()


def _list_remote_json_files(token: t.Optional[str]) -> tuple[str, set[str]]:
    """Return (default_branch, set_of_json_paths) where paths are like 'data/.../*.json'."""
    repo_api = "https://api.github.com/repos/statsbomb/open-data"
    repo = _github_get_json(repo_api, token)
    default_branch = repo.get("default_branch", "master")
    br = _github_get_json(f"{repo_api}/branches/{default_branch}", token)
    tree_sha = (((br.get("commit") or {}).get("commit") or {}).get("tree") or {}).get("sha")
    if not tree_sha:
        raise RuntimeError("Could not determine tree sha for default branch")
    tree = _github_get_json(f"{repo_api}/git/trees/{tree_sha}?recursive=1", token)
    paths: set[str] = set()
    for item in (tree.get("tree") or []):
        if item.get("type") == "blob":
            path = item.get("path") or ""
            if path.startswith("data/") and path.endswith(".json"):
                paths.add(path)
    return default_branch, paths


@app.command()
def sync_statsbomb_data(
    data_dir: str = DEFAULT_DATA_DIR,
    check_only: bool = typer.Option(True, "--check-only/--sync", help="When --sync, download missing JSON files."),
    token: t.Optional[str] = typer.Option(None, help="GitHub token to raise rate limits (falls back to GITHUB_TOKEN)"),
):
    """Check remote StatsBomb Open Data JSONs against local archive and optionally download missing ones.

    - Compares files under remote 'data/' with local 'archive/data/'.
    - With --sync, downloads missing files via raw.githubusercontent.com without overwriting existing files.
    """
    token = token or os.environ.get("GITHUB_TOKEN")
    typer.echo("Querying StatsBomb open-data repository metadata...")
    branch, remote_paths = _list_remote_json_files(token)
    typer.echo(f"Remote JSON files: {len(remote_paths)} on branch '{branch}'")

    # Build local set with 'data/' prefix to match remote shape
    local_set: set[str] = set()
    for root, _dirs, files in os.walk(data_dir):
        for fn in files:
            if not fn.endswith(".json"):
                continue
            full = os.path.join(root, fn)
            rel = os.path.relpath(full, data_dir)  # e.g. 'events/123.json'
            local_set.add(f"data/{rel.replace(os.sep, '/')}")

    missing = sorted(remote_paths - local_set)
    typer.echo(f"Missing locally: {len(missing)}")
    # Print a small sample for visibility
    for p in missing[:20]:
        typer.echo(f"  - {p}")
    if len(missing) > 20:
        typer.echo(f"  ... and {len(missing) - 20} more")

    if check_only or not missing:
        if not missing:
            typer.echo("Local dataset is up to date. Nothing to do.")
        else:
            typer.echo("Check-only mode: no files will be downloaded. Use --sync to apply.")
        return

    # Download missing files
    base_raw = f"https://raw.githubusercontent.com/statsbomb/open-data/{branch}"
    downloaded = 0
    for path in missing:
        # path like 'data/events/123.json' -> local: data_dir/events/123.json
        relative_under_data = path.split('/', 1)[1]
        local_path = os.path.join(data_dir, relative_under_data)
        if os.path.exists(local_path):
            continue  # shouldn't happen, but be safe
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        url = f"{base_raw}/{path}"
        try:
            r = requests.get(url, timeout=60)
            if r.status_code != 200:
                typer.echo(f"[WARN] Failed {url}: {r.status_code}")
                continue
            with open(local_path, "wb") as f:
                f.write(r.content)
            downloaded += 1
            if downloaded % 50 == 0:
                typer.echo(f"Downloaded {downloaded}/{len(missing)}...")
        except Exception as e:
            typer.echo(f"[WARN] Error downloading {url}: {e}")
            continue

    typer.echo(f"Download complete. Downloaded {downloaded} files. Skipped {len(missing) - downloaded} (errors).")


@app.command()
def stream_import_statsbomb(
    categories: str = typer.Option(
        "competitions,matches,lineups,events",
        "--categories",
        help="Comma-separated categories to stream: competitions,matches,lineups,events",
    ),
    max_per_category: int = typer.Option(50, "--max-per-category", "-n", help="Max files to process per category this run"),
    token: t.Optional[str] = typer.Option(None, help="GitHub token to raise rate limits (falls back to GITHUB_TOKEN)"),
):
    """Stream-import StatsBomb data directly from GitHub without keeping local copies.

    Logic per category:
    - competitions: download competitions.json once, import countries and competitions/seasons, mark processed.
    - matches: download unprocessed matches/*/*.json, import teams+stadiums+matches, mark processed with path 'matches/<comp>/<season>.json'.
    - lineups: download unprocessed lineups/<match_id>.json, import players/squads+lineups, mark processed with basename '<match_id>.json'.
    - events: download unprocessed events files and call import_events, which self-marks processed files by basename.
    """
    token = token or os.environ.get("GITHUB_TOKEN")
    req = {c.strip().lower() for c in (categories or "").split(",") if c.strip()}
    if not req:
        typer.echo("No categories specified; nothing to do.")
        return

    typer.echo("Querying StatsBomb open-data repository metadata...")
    branch, remote_paths = _list_remote_json_files(token)
    base_raw = f"https://raw.githubusercontent.com/statsbomb/open-data/{branch}"

    # Partition remote paths
    comps_path = "data/competitions.json" if "data/competitions.json" in remote_paths else None
    match_paths = sorted([p for p in remote_paths if p.startswith("data/matches/") and p.endswith(".json")])
    lineup_paths = sorted([p for p in remote_paths if p.startswith("data/lineups/") and p.endswith(".json")])
    event_paths = sorted([p for p in remote_paths if p.startswith("data/events/") and p.endswith(".json")])

    processed_counts: dict[str, int] = {"competitions": 0, "matches": 0, "lineups": 0, "events": 0}

    with get_conn() as conn:
        with conn.cursor() as cur:
            # Ensure table exists
            cur.execute(SQL_CREATE_PROGRESS_TABLE)
            conn.commit()

            # competitions
            if "competitions" in req and comps_path:
                cur.execute(SQL_LIST_PROCESSED, ('competitions',))
                processed = {row[0] for row in cur.fetchall()}
                if "competitions.json" not in processed:
                    typer.echo("[competitions] Importing competitions.json ...")
                    with tempfile.TemporaryDirectory() as tmpdir:
                        local_rel = comps_path.split("/", 1)[1]  # 'competitions.json'
                        local_path = os.path.join(tmpdir, local_rel)
                        os.makedirs(os.path.dirname(local_path), exist_ok=True)
                        url = f"{base_raw}/{comps_path}"
                        r = requests.get(url, timeout=60)
                        r.raise_for_status()
                        with open(local_path, "wb") as f:
                            f.write(r.content)
                        # Import
                        import_countries(data_dir=tmpdir)
                        import_competitions_and_seasons(data_dir=tmpdir)
                    cur.execute(SQL_MARK_PROCESSED, ('competitions', 'competitions.json'))
                    conn.commit()
                    processed_counts["competitions"] += 1
                else:
                    typer.echo("[competitions] Already processed. Skipping.")

            # matches
            if "matches" in req and match_paths:
                cur.execute(SQL_LIST_PROCESSED, ('matches',))
                processed = {row[0] for row in cur.fetchall()}  # store rel like 'matches/<comp>/<season>.json'
                # Build missing list limited
                missing: list[str] = []
                for p in match_paths:
                    rel = p.split('/', 1)[1]  # 'matches/...json'
                    if rel not in processed:
                        missing.append(rel)
                        if len(missing) >= max_per_category:
                            break
                typer.echo(f"[matches] To process: {len(missing)}")
                for rel in missing:
                    url = f"{base_raw}/data/{rel}"
                    with tempfile.TemporaryDirectory() as tmpdir:
                        local_path = os.path.join(tmpdir, rel)
                        os.makedirs(os.path.dirname(local_path), exist_ok=True)
                        try:
                            r = requests.get(url, timeout=60)
                            if r.status_code != 200:
                                typer.echo(f"[WARN] Failed {url}: {r.status_code}")
                                continue
                            with open(local_path, "wb") as f:
                                f.write(r.content)
                            # Import the minimal dependencies then the match
                            import_teams(data_dir=tmpdir)
                            import_stadiums(data_dir=tmpdir)
                            import_matches(data_dir=tmpdir)
                            # Mark processed
                            cur.execute(SQL_MARK_PROCESSED, ('matches', rel))
                            conn.commit()
                            processed_counts["matches"] += 1
                        except Exception as e:
                            typer.echo(f"[ERROR] matches import failed for {rel}: {e}")

            # lineups
            if "lineups" in req and lineup_paths:
                cur.execute(SQL_LIST_PROCESSED, ('lineups',))
                processed = {row[0] for row in cur.fetchall()}  # store basename '123.json'
                # Build missing list limited
                missing: list[tuple[str, str]] = []  # (rel_under_data, basename)
                for p in lineup_paths:
                    rel = p.split('/', 1)[1]  # 'lineups/123.json'
                    bname = os.path.basename(rel)
                    if bname not in processed:
                        missing.append((rel, bname))
                        if len(missing) >= max_per_category:
                            break
                typer.echo(f"[lineups] To process: {len(missing)}")
                for rel, bname in missing:
                    url = f"{base_raw}/data/{rel}"
                    with tempfile.TemporaryDirectory() as tmpdir:
                        local_path = os.path.join(tmpdir, rel)
                        os.makedirs(os.path.dirname(local_path), exist_ok=True)
                        try:
                            r = requests.get(url, timeout=60)
                            if r.status_code != 200:
                                typer.echo(f"[WARN] Failed {url}: {r.status_code}")
                                continue
                            with open(local_path, "wb") as f:
                                f.write(r.content)
                            # Import players/squads then lineups
                            import_players_and_squads(data_dir=tmpdir)
                            import_lineups(data_dir=tmpdir)
                            # Mark processed (basename to mirror events style)
                            cur.execute(SQL_MARK_PROCESSED, ('lineups', bname))
                            conn.commit()
                            processed_counts["lineups"] += 1
                        except Exception as e:
                            typer.echo(f"[ERROR] lineups import failed for {rel}: {e}")

            # events
            if "events" in req and event_paths:
                cur.execute(SQL_LIST_PROCESSED, ('events',))
                processed = {row[0] for row in cur.fetchall()}  # basenames like '123.json'
                missing: list[str] = []  # rel_under_data
                for p in event_paths:
                    rel = p.split('/', 1)[1]  # 'events/...json'
                    bname = os.path.basename(rel)
                    if bname not in processed:
                        missing.append(rel)
                        if len(missing) >= max_per_category:
                            break
                typer.echo(f"[events] To process: {len(missing)}")
                for rel in missing:
                    url = f"{base_raw}/data/{rel}"
                    with tempfile.TemporaryDirectory() as tmpdir:
                        local_path = os.path.join(tmpdir, rel)
                        os.makedirs(os.path.dirname(local_path), exist_ok=True)
                        try:
                            r = requests.get(url, timeout=60)
                            if r.status_code != 200:
                                typer.echo(f"[WARN] Failed {url}: {r.status_code}")
                                continue
                            with open(local_path, "wb") as f:
                                f.write(r.content)
                            # Call import_events which self-marks processed by basename
                            import_events(data_dir=tmpdir, resume=True)
                            processed_counts["events"] += 1
                        except Exception as e:
                            typer.echo(f"[ERROR] events import failed for {rel}: {e}")

    typer.echo("== Stream Import Summary ==")
    for cat, cnt in processed_counts.items():
        typer.echo(f"- {cat}: {cnt} files processed")


@app.command()
def stage1(data_dir: str = DEFAULT_DATA_DIR):
    """Run Stage 1: countries -> competitions/seasons -> teams -> stadiums -> matches"""
    import_countries(data_dir=data_dir)
    import_competitions_and_seasons(data_dir=data_dir)
    import_teams(data_dir=data_dir)
    import_stadiums(data_dir=data_dir)
    import_matches(data_dir=data_dir)


@app.command()
def status(data_dir: str = DEFAULT_DATA_DIR):
    """Print import status: table counts and events progress."""
    # Count JSON files for events
    total_event_files = 0
    total_event_files += len(glob.glob(os.path.join(data_dir, "events", "*.json")))
    total_event_files += len(glob.glob(os.path.join(data_dir, "events", "*", "*.json")))

    with get_conn() as conn:
        with conn.cursor() as cur:
            def count(tbl: str) -> int:
                cur.execute(f'SELECT COUNT(*) FROM "{tbl}"')
                return int(cur.fetchone()[0])

            # Core tables
            ulkeler = count("ulkeler")
            kars = count("karsilasmalar")
            sezon = count("sezonlar")
            takim = count("takimlar")
            stady = count("stadyumlar")
            mac = count("maclar")

            # Stage 2
            oyuncu = count("oyuncular")
            kadro = count("kadrolar")
            dizilis = count("dizilisler")

            # Stage 3
            olay = count("olaylar")
            # processed files
            cur.execute('SELECT COUNT(*) FROM "islenen_dosyalar" WHERE kategori = %s', ('events',))
            processed_events = int(cur.fetchone()[0]) if cur.rowcount is not None else 0

            # Breakdown by event type
            cur.execute('SELECT olay_tipi, COUNT(*) FROM "olaylar" GROUP BY olay_tipi ORDER BY 1')
            breakdown = cur.fetchall()

    typer.echo("== Import Status ==")
    typer.echo(f"- Countries (ulkeler): {ulkeler}")
    typer.echo(f"- Competitions (karsilasmalar): {kars}")
    typer.echo(f"- Seasons (sezonlar): {sezon}")
    typer.echo(f"- Teams (takimlar): {takim}")
    typer.echo(f"- Stadiums (stadyumlar): {stady}")
    typer.echo(f"- Matches (maclar): {mac}")
    typer.echo("")
    typer.echo(f"- Players (oyuncular): {oyuncu}")
    typer.echo(f"- Squads (kadrolar): {kadro}")
    typer.echo(f"- Lineups (dizilisler): {dizilis}")
    typer.echo("")
    typer.echo(f"- Events (olaylar): {olay}")
    typer.echo(f"- Events files processed: {processed_events} / {total_event_files}")
    remaining = max(0, total_event_files - processed_events)
    typer.echo(f"- Remaining event files: {remaining}")
    if breakdown:
        typer.echo("- Events breakdown:")
        for tip, cnt in breakdown:
            typer.echo(f"  * {tip}: {cnt}")


@app.command()
def stage2(data_dir: str = DEFAULT_DATA_DIR):
    """Run Stage 2: players -> squads -> lineups"""
    import_players_and_squads(data_dir=data_dir)
    import_lineups(data_dir=data_dir)


@app.command()
def stage3(data_dir: str = DEFAULT_DATA_DIR):
    """Run Stage 3: events"""
    import_events(data_dir=data_dir)


if __name__ == "__main__":
    app()
